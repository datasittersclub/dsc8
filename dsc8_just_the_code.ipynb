{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Sitters Club 8: Just the Code\n",
    "\n",
    "This notebook contains just the code (and a little bit of text) from the portions of *[DSC 8: Text-Comparison-Algorithm-Crazy-Quinn](https://datasittersclub.github.io/site/dsc8/)* for using Euclidean and cosine distance with word counts and word frequencies, and running TF-IDF for your texts.\n",
    "\n",
    "This code assumes you've actually read the Data-Sitters Club book already. There's lots of pitfalls if you just try to apply the code without understanding what it's doing, or the effect caused by the various different options. Read first, then try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installs seaborn\n",
    "#You only need to run this cell the first time you run this notebook\n",
    "import sys\n",
    "!{sys.executable} -m pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports the count vectorizer from Scikit-learn along with \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Glob is used for finding path names\n",
    "import glob\n",
    "#We need these to format the data correctly\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "#In case you're starting to run the code just at this point, we'll need os again\n",
    "import os\n",
    "import numpy as np\n",
    "#In case you're starting to run the code just at this point, we'll need pandas again\n",
    "import pandas as pd\n",
    "#Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#Import seaborn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the file directory for your corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir = '/Users/qad/Documents/dsc_corpus_clean'\n",
    "os.chdir(filedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word count vectorizer\n",
    "This looks at just the top 1000 words, and doesn't use `max_df` to remove words that occur across all your texts. You can add it in between the input and the `max_features` parameters, separated by a comma (e.g. `input=\"filename\", max_df=.7, max_features=1000`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the glob library to create a list of file names, sorted alphabetically\n",
    "# Alphabetical sorting will get us the books in numerical order\n",
    "filenames = sorted(glob.glob(\"*.txt\"))\n",
    "# Parse those filenames to create a list of file keys (ID numbers)\n",
    "# You'll use these later on.\n",
    "filekeys = [f.split('/')[-1].split('.')[0] for f in filenames]\n",
    "\n",
    "# Create a CountVectorizer instance with the parameters you need\n",
    "wordcountvectorizer = CountVectorizer(input=\"filename\", max_features=1000)\n",
    "# Run the vectorizer on your list of filenames to create your wordcounts\n",
    "# Use the toarray() function so that SciPy will accept the results\n",
    "wordcounts = wordcountvectorizer.fit_transform(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: word count toy\n",
    "The code below will display all the words that were included in the word count vectorizer, based on the parameters you've set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words = wordcounts.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in wordcountvectorizer.vocabulary_.items()]\n",
    "sorted(words_freq, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean distance for word count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs the Euclidean distance calculation, prints the output, and saves it as a CSV\n",
    "euclidean_distances = pd.DataFrame(squareform(pdist(wordcounts)), index=filekeys, columns=filekeys)\n",
    "euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines the size of the image\n",
    "plt.figure(figsize=(100, 100))\n",
    "#Increases the label size so it's more legible\n",
    "sns.set(font_scale=3)\n",
    "#Generates the visualization using the data in the dataframe\n",
    "ax = sns.heatmap(euclidean_distances)\n",
    "#Displays the image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine distance for word count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances = pd.DataFrame(squareform(pdist(wordcounts, metric='cosine')), index=filekeys, columns=filekeys)\n",
    "cosine_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine distance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines the size of the image\n",
    "plt.figure(figsize=(100, 100))\n",
    "#Increases the label size so it's more legible\n",
    "sns.set(font_scale=3)\n",
    "#Generates the visualization using the data in the dataframe\n",
    "ax = sns.heatmap(cosine_distances)\n",
    "#Displays the image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term frequency vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use the glob library to create a list of file names, sorted alphabetically\n",
    "# Alphabetical sorting will get us the books in numerical order\n",
    "filenames = sorted(glob.glob(\"*.txt\"))\n",
    "# Parse those filenames to create a list of file keys (ID numbers)\n",
    "# You'll use these later on.\n",
    "filekeys = [f.split('/')[-1].split('.')[0] for f in filenames]\n",
    "\n",
    "# Create a CountVectorizer instance with the parameters you need\n",
    "freqvectorizer = TfidfVectorizer(input=\"filename\", stop_words=None, use_idf=False, norm='l1', max_features=1000)\n",
    "# Run the vectorizer on your list of filenames to create your wordcounts\n",
    "# Use the toarray() function so that SciPy will accept the results\n",
    "wordfreqs = freqvectorizer.fit_transform(filenames).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean distance for term frequency vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distances_freq = pd.DataFrame(squareform(pdist(wordfreqs, metric='euclidean')), index=filekeys, columns=filekeys)\n",
    "euclidean_distances_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines the size of the image\n",
    "plt.figure(figsize=(100, 100))\n",
    "#Increases the label size so it's more legible\n",
    "sns.set(font_scale=3)\n",
    "#Generates the visualization using the data in the dataframe\n",
    "ax = sns.heatmap(euclidean_distances_freq)\n",
    "#Displays the image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine distance for word count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances_freq = pd.DataFrame(squareform(pdist(wordfreqs, metric='cosine')), index=filekeys, columns=filekeys)\n",
    "cosine_distances_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine distance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines the size of the image\n",
    "plt.figure(figsize=(100, 100))\n",
    "#Increases the label size so it's more legible\n",
    "sns.set(font_scale=3)\n",
    "#Generates the visualization using the data in the dataframe\n",
    "ax = sns.heatmap(cosine_distances_freq)\n",
    "#Displays the image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the glob library to create a list of file names, sorted alphabetically\n",
    "# Alphabetical sorting will get us the books in numerical order\n",
    "filenames = sorted(glob.glob(\"*.txt\"))\n",
    "# Parse those filenames to create a list of file keys (ID numbers)\n",
    "# You'll use these later on.\n",
    "filekeys = [f.split('/')[-1].split('.')[0] for f in filenames]\n",
    "\n",
    "# Create a CountVectorizer instance with the parameters you need\n",
    "vectorizer = TfidfVectorizer(input=\"filename\", stop_words=None, use_idf=True, norm=None, max_features=1000, max_df=.95)\n",
    "# Run the vectorizer on your list of filenames to create your wordcounts\n",
    "# Use the toarray() function so that SciPy will accept the results\n",
    "transformed_documents = vectorizer.fit_transform(filenames)\n",
    "transformed_documents_as_array = transformed_documents.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a CSV per text file with most distinctive terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a list of output file paths using the previous list of text files the relative path for tf_idf_output\n",
    "output_filenames = [str(txt_file).replace(\".txt\", \".csv\") for txt_file in filenames]\n",
    "\n",
    "# loop each item in transformed_documents_as_array, using enumerate to keep track of the current position\n",
    "for counter, doc in enumerate(transformed_documents_as_array):\n",
    "    # construct a dataframe\n",
    "    tf_idf_tuples = list(zip(vectorizer.get_feature_names(), doc))\n",
    "    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # output to a csv using the enumerated value for the filename\n",
    "    one_doc_as_df.to_csv(output_filenames[counter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested Citation\n",
    "\n",
    "Dombrowski, Quinn. “DSC #8: Just the Code.” Jupyter Notebook. *The Data-Sitters Club*, October 21, 2020. https://github.com/datasittersclub/dsc8."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
